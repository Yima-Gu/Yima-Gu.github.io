---
title: Deep Learning Lecture-5
date: 2025-06-21
tags:
  - DeepLearning
math: true
---

## Recurrent Network

### Sequence Model

序列建模任务是指对一个序列的输入进行建模，比如文本、音频、视频等。序列模型的输入和输出都是序列，比如机器翻译、语音识别、视频分类等任务。重要的是**捕捉序列中的上下文**。

### Basic Principle

#### Local Dependency

**Local Dependency**：对于一个序列中的每一个元素，它的预测是依赖于它的前面的元素的。这种依赖关系是**局部的**。

$$ 
P(x_1,x_2,\dots,x_T) = \prod_{t=1}^{T} P(x_t|x_1,\dots,x_{t-1}) = 
\prod_{t=1}^{T} g(s_{t-2},x_{t-1})
$$

如果引入马尔科夫性，那么损失的信息太多了。因此我们引入一个隐藏状态$s_t$，$s_t$的信息是前面的元素的信息的一个编码。
假设第$t$时间的元素的信息都被编码到了$s_{t-2}$和$x_{t-1}$中，$s_{t-2}$是一个向量，$g$是一个函数，$s_{t-2}$是一个隐藏状态。我们认为时间上也存在一个感受野。这个思考过程和人也是类似的，在处理序列问题时，也保存早些时候的一些信息。

另外也有时间上的**平稳性假设**，这个假设是人工智能能预测未来的理论基础。本质是时间上的独立同分布假设。

#### Parametric Sharing

不同时刻使用的参数是一样的，这样可以大大降低参数量。这样的模型是**循环神经网络**。

#### Language Model

语言中的建模任务为：给定一个句子的前面的单词，预测下一个单词。这个任务被称为语言模型。语言模型的目标是最大化句子的概率。语言模型的输入是一个句子，输出是一个概率分布，表示下一个单词的概率。

**向量化表达**：可以使用one-hot向量表示单词，也可以使用词向量表示单词。每个词的表达大概需要1000维度。如果使用MLP，要将每个词的向量拼接起来，然后输入到MLP中。这样的模型不适合处理长序列，参数是非常可怕的。在MLP中丢失了一部分的信息（由于MLP的输入维度是可交换的），丢失了前后序关系。

#### A Better MLP

**n-gram**：使用n-gram模型，可以考虑前n个单词的信息。

![[Pasted image 20250320225821.png]]
输出的概率分布是一个softmax层，输入是一个向量，这个向量是前n个单词的向量拼接起来的。然后在得到的概率分布上进行采样。**滚动预测**，使用第一个次的预测结果作为第二次的输入，会有**误差累计**的问题。
但是上述模型会有一个问题：参数量太大；对于每一个词的预测需要有一个MLP，这样的模型不适合处理长序列。
### RNN

![[Pasted image 20250321083321.png]]

循环神经网络中最重要的内容是中间的隐藏层，构建学习到的时间程度上的特征。在每个时刻输入的都是向量，称为*token embedding*。
上述模型在纵向方向上，从$x_t$到$y_t$就是一个前馈网络*feedforward network*（包括MLP和CNN）。在横向方向上，从$s_{t-1}$到$s_t$就是一个循环网络*recurrent network*。
#### Recurrent Layer
$h_t$用来编码$t$时刻之前的所有信息。对于这样的层，接受的输入是$x_t$和$h_{t-1}$，输出是$h_t$。$h_t$的计算公式如下：
$$
h_t = f_W(h_{t-1},x_t) 
$$
$$
h_t = \tanh (Wh_{t-1} + Ux_t)
$$
长期以来使用的是双曲正切作为激活函数，但是使用ReLu可能有更好的梯度性质。
$$
y_t = Vh_t
$$
可以认为$h_t$包含了之前的所有信息，所以使用$h_t$来预测$y_t$。**通过引入状态变量来使得递推公式在形式上是二阶依赖。**

#### Bidirectional&Deep RNN

![[Pasted image 20250321091923.png]]

![[Pasted image 20250321091936.png]]

这里纵向方向上的前馈网络中的训练难点在前面的MLP与CNN中是一样的，梯度消失和梯度爆炸。
上面的图中的$y_{t,c}$是真是标签的独热编码，$C$是此表中的元素个数。

在横向方向上，梯度消失是很严重的。因为$h_t$包含了$h_{t-1}$的信息，所以梯度会在时间上指数级的衰减。解决这个问题的方法是**LSTM**和**GRU**。

#### RNN for LM

- 理论上，可以表达没有边界的时间上的依赖，由于将状态变量编码为$h_t$，$h_t$包含了之前的所有信息。
- 将序列编码到一个向量中，这个向量包含了整个序列的信息。
- 参数在时间上是共享的
- 但是在实际上，很难建模时间上的长时间依赖。对于较早的信息，后面的权重会很小。

**一个模型是否有效，在于*assumptions*与实际情况是否匹配。**

#### Architecture

![[Pasted image 20250321102900.png]]

##### Many to One

主要实现的是情感识别、文本分类等任务。在最后一个时间步的输出是最终的输出。最后一个时刻的状态不一定包含有重要的信息（比如上下文中的情感词）。

##### One to Many

可能的输入有，比如输入一个图像的特征向量再输入到这个网络中。对于这个输入，应该是输入每个状态还是输入所有的状态。并没有解决每一个词对应图中的哪一个区域的问题。

##### Many to Many

有两种情况，输入是每个时刻的语音的因素，输出的是对应的symbol，是输入输出平行*parallel*的，但是输入和输出是异构的。

LM输入和输出是基本上平行的，但是滞后一个时刻，是自回归*autoregressive*的。
##### Sequence to Sequence

输入和输出都是序列，输入和输出的长度不一定相同。比如机器翻译、语音识别等任务。这个任务可以分为两个部分：编码器和解码器。编码器将输入序列编码到一个向量中，解码器将这个向量解码到输出序列中。

![[Pasted image 20250321110238.png]]

*为什么上述序列中的参数$W_1$和$W_2$是不一样的？*

首先将输入变量$\{x_t\}$编码到状态变量$\{h_t\}$中，然后再将状态变量$\{h_t\}$解码到输出变量$\{y_t\}$中。编码器是没有loss function的，因为输入和输出是异构的。解码器接受的输入是编码器的输出，解码器的输出是一个概率分布。解码器的loss function是交叉熵损失函数。

**机器翻译任务中的挑战**：
- 输入和输出是异构的
- 长序列的处理

![[Pasted image 20250321130443.png]]

**如何从概率中采样**：
- 选择概率最大的
- 概率较大的有更大的概率被选择
- Beam Search贪心方法进行搜索

![[Pasted image 20250321130842.png]]

可以选择概率较大的k个词，然后以这个词为条件计算下一个词的条件概率，类似于构建一个真k叉树，这样的方法是一种贪心的方法。考虑这棵树上所有的路径，选择最大的路径（本质上是一种搜索技术）。

### Backpropagation Through Time

![[Pasted image 20250321133148.png]]

$$
\frac{\partial L}{\partial U}  = \sum_{t=0}^T \frac{\partial L_{t}}{\partial U} = \sum_{t=0}^{T} \sum_{s=0}^t \frac{\partial L_{t}}{\partial y_t} \frac{\partial y_t}{\partial h_t}  \frac{\partial h_t}{\partial h_s}\frac{\partial h_s}{\partial U}
$$
前一个求和的意义是对于损失函数的各个部分求和，后面的求和式是对于$h_t$的前面的每一个可能的链求和。
其中：
$$
\frac{\partial h_t}{\partial h_s} = \prod_{i=s+1}^t \frac{\partial h_i}{\partial h_{i-1}}
$$
这个式子是一个矩阵乘法，是一个矩阵的连乘。这个矩阵是一个雅可比矩阵。

![[Pasted image 20250321135457.png]]

用*Cauchy-Schwarz*不等式可以证明：
$$
\| \frac{\partial h_t}{\partial h_{t-1}} \| \leq \| W^T \| \|diag (f'(h_{t-1}))\| \leq \sigma_{max} \gamma
$$
这里$\sigma_{max}$是矩阵$W$的最大奇异值，$\gamma$是激活函数的导数的最大值。
于是：
$$
\| \frac{\partial h_t}{\partial h_{s}} \| =  \prod_{i=s+1}^t \| \frac{\partial h_i}{\partial h_{i-1}} \| \leq (\sigma_{max} \gamma)^{t-s}
$$
这个式子说明了梯度消失的问题，梯度消失是指梯度在时间上的指数级衰减。或者梯度爆炸的问题，梯度爆炸是指梯度在时间上的指数级增长。

#### Truncated BPTT

![[Pasted image 20250321140454.png]]

这个方法是将时间上的梯度截断，这样可以减少梯度消失和梯度爆炸的问题。但是这样的方法会导致梯度的估计不准确，因为梯度的估计是基于一个截断的时间窗口的。

#### Long Short-Term Memory

> 为什么这样就能实现所谓的LSTM


- 遗忘，将过去“没用”的信息遗忘
- 更新，将新的信息更新到状态变量中
- 输出，输出门控制一部分信息用来进行预测
![[Pasted image 20250321141243.png]]
$t$时刻的状态变量$h_t$储存的是$t$时刻的信息，$c_t$是$t$时刻的记忆变量，$h_{t-1}$和$x_t$是$t$时刻的输入，$f_t$是遗忘门，$i_t$是输入门，$o_t$是输出门，$g_t$是更新门。

![[Pasted image 20250321141234.png]]
上述网络构造了一个信息流高速路径，使得梯度能够进行快速的传播。

遗忘门和残差网络的思想是类似的，都是将过去的信息和现在的信息进行融合。这样的网络可以更好的处理长序列的问题。[[Deep Learning Lecture-3#ResNet]]

#### Gradient Clipping

梯度的大小是由模长决定的，如果梯度的模长过大，可以将梯度的模长进行截断。这样可以避免梯度爆炸的问题。

#### Variational Dropout

在深度网络中，如果是过拟和的，也就是对于一个含有多个参数的网络。也就是说如果输入的参数小于参数的个数，那么相对应的线性方程组是欠定的。

在RNN中对应的纵向方向上是多层感知机，所以可以采用标准的Dropout方法。但是在横向方向上是一个循环网络，Dropout方法不适用。因为Dropout方法会破坏时间上的连续性，违背了参数共享的原则。
采用**步调一致**的方法进行操作，这样可以保持时间上的连续性。这样的方法是**Variational Dropout**。

#### Layer Normalization

![[Pasted image 20250322202759.png]]

在CNN中，对于每一个通道的值进行归一化。在这样的每一个通道中计算均值和方差。还是要加入一个平移变量和伸缩变量。

在RNN中，主要的原因是门控结构是相对于每一个序列而言的，所以应该引入一种新的归一化方法*Layer Normalization*，应该在每一条样本（一个序列）在每一个时刻经过之后的值在$C$个通道上进行归一化操作。
最后得到的结果是：将所有的向量放在以原点为球心的单位球面上。

#### Weight Normalization

对于每层的参数$\mathbf{w}$进行重参数化：
$$
\mathbf{w} = \frac{g}{\|v\|}v
$$
![[Pasted image 20250322204925.png]]

对右边的式子$\frac{v}{\|v\|}$进行优化是更为容易的，重参数化的意思是，在训练和测试的时候使用不同的参数表达形式，这样是更加容易优化的。

#### Training-Inference Shift

滚动预测：在训练的时候，使用真实的标签进行预测；在测试的时候，使用的是推理得到的值进行预测。这是一个自回归任务。

##### Curriculum Learning

在训练的时候，可以先训练一些简单的任务，然后再训练一些复杂的任务。这样可以更好的训练模型。
在实际中，可以对所有的样本计算loss，先计算loss较小的样本，然后再计算loss较大的样本。这就是**自步学习**。
这里涉及到选择不同的样本顺序的问题。

*Scheduled Sampling*：在学习刚开始的时候，更多地使用真实的标签进行预测；随着学习的进行，更多地使用模型预测的值进行预测。**是RNN中很重要的技术**。

### RNN with Attention

#### Human Attention
人类的注意力：
- 可持续注意力（没有实现）
- 选择性注意力（人类的选择性注意力复杂得多）
- 交替式注意力
- 分配式注意力

#### Attention in Deep Learning

> *Allowing the model to dynamically pay attention to only certain parts of the input that help in performing the task at hand effectively.*

存在时间*temporal Attention*和空间*Spatial Attention*上的注意力。一般而言指的是时间上的注意力。

#### Auto-Regessive

[[#Sequence to Sequence]]

最重要的问题是编码器和解码器之间的信息沟通太少了，存在有信息瓶颈。并且在翻译任务中，输入和输出的顺序并不是一致的，大部分的语言的语序是不一样的。
**希望看到后面的信息**，这和RNN的设计目的是相违背的。**全连接的思想**又回来了，获得全局信息的方法有很多，不只是有MLP的方法。有一种基本思想是***Relevance***，也就是和当前任务相关的信息。这个思想是在Attention中得到了体现。

#### Attention

![[Pasted image 20250322222251.png]]

计算两个东西的相似度有：计算内积、输入*relation network*。这样的模型在互联网中有很多的应用，比如推荐系统、搜索引擎等。

![[Pasted image 20250323084415.png]]

注意力的分配是符合概率分布的，所以可以使用上面计算得到的相关性$e_{ij}$使用softmax函数进行归一化。这样得到的分布就是注意力的分布：
$$
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}
$$
上述计算式表达的含义是，在状态$i$的时刻分配在$j$上的注意力（对于$j$的求和为1）。继续计算$c_i$：
$$
c_i = \sum_{j=1}^{T_x} \alpha_{ij} x_j
$$
这里$c_i$是对于状态$i$的时刻的注意力向量，是对于$x$的加权和。
$$
s_i = f(s_{i-1},y_{i-1},c_i)
$$
这里$s_i$是状态变量，$y_{i-1}$是前一个时刻的输出，$c_i$是当前时刻的注意力向量。这里的函数$f$是一个GRU or LSTM。这个模型是一个**Seq2Seq**模型。

> 这里的$c_i$和$s_i$的区别是什么，为什么和LSTM有关

![[Pasted image 20250323084332.png]]
![[Pasted image 20250323084535.png]]

在机器翻译的过程中，使用source的上下文信息比直接使用词典来翻译更好。这样的模型可以更好的处理长序列的问题。只要是序列都会使用**滑动窗口**，一般设置为50~100之间。对于较短的情况，可以使用psdding的方法；对于较长的情况会使用截断的方法。希望找一个与序列的长度线性关系的模型。

#### Attention vs. MLP

相同点：
- 都是全局模型，是对于长序关系的建模。

不同点：
- Attention是基于概率的，MLP是基于全连接的。
- Attention引入relevance的思想，能大大减小参数量

#### Hierarchical Attention

先建模词注意力然后再建模句子注意力。

#### Global Attention

$$
\text{score} = \begin{cases}
h_t^T \overline{h_s} \\
h_t^T W_a \overline{h_s} \\
v_a^T \tanh(W_a[h_t;\overline{h_s}])
\end{cases}
$$
发现上面三种计算方式的效率是差不多的。

![[Pasted image 20250323090740.png]]

### Memory

#### Human Memory

- Sensory Memory
	- 计算机视觉与机器感知
- Short-term Memory
	- 与计算机中的内存是很相近的，LSTM是一种将短期记忆尽量变长的方法。
- Long-term Memory
	- 前面的模型中没有实现这个功能

在自然语言中，比较困难的任务是进行对话，这时候需要进行长期记忆。在对话中，需要对话的上下文进行理解。

#### Neural Turing Machine

![[Pasted image 20250323092327.png]]

在这个模型中，最重要的是对内存1进行寻址的操作，这个操作是一个注意力的操作。
对于读的操作，是按照注意力的大小对地址里面的内容进行加权平均。
对于写的操作，类似于LSTM中的Forget Gate，先进行擦除之后才进行写入。

对于Internal Memory，最大的问题是可能会遗忘，对于External Memory，是一个外部的存储器，这样的存储是比较稳定的。